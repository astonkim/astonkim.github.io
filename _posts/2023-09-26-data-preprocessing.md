---
layout: post
title: ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ ê°œë¡  Study Note - 1
subtitle: 01. Data Preprocessing
categories: DGU Bachelor of Data Science
tags: [ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤_ê°œë¡ ]
---

## Data Mining Process
## Pipeline

- The workflow of a typical data mining application
	- **Data collection**  (ë°ì´í„° ìˆ˜ì§‘)
	- **Pre-processing**  (ì „ì²˜ë¦¬: í™œìš©ì„ ìœ„í•œ ì •ì œ)
	- **Analytical processing and algorithms** (ë¶„ì„ ì²˜ë¦¬ì™€ ì•Œê³ ë¦¬ì¦˜)
	- **Post-processing** (í›„ì²˜ë¦¬)

![](https://i.imgur.com/QOnQFUF.png)

<br/>

## Major Tasks in Data Preprocessing

### Data cleaning  

- Fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies (ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì–´ ìˆëŠ” ê²½ìš°ì™€ ì˜ëª» ê¸°ë¡ëœ ê²½ìš°ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€)

### Data integration

- Integration of multiple databases, data cubes, or files (ì—¬ëŸ¬ ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, ì¼ê´€ì„±ì„ í•´ì¹˜ì§€ ì•ŠëŠ” ë°©ë²•)

### Data reduction

- Dimensionality reduction, Data compression (ë°ì´í„°ê°€ ë„ˆë¬´ í´ ê²½ìš° ì–´ë–»ê²Œ ì¤„ì´ëŠ”ê°€)

### Data transformation

- Discretization, Normalization (ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë³€í˜•ì‹œí‚¤ëŠ”ê°€)

<br/>

## Data Cleaning

> **ğŸ“™ NOTE: Data Quality: Why Preprocess the Data?**
> - **Measures for data quality**: A multidimensional view
> - **Accuracy**: ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë°ì´í„°ê°€ ìˆ˜ì§‘ì´ ë˜ì—ˆëŠ”ì§€. correct or wrong, accurate or not
> - **Completeness**: ê¸°ë¡ì´ ì˜ ì•ˆëœ ê²ƒë“¤ì€ ì—†ëŠ”ì§€ not recorded, unavailable, ...
> - **Consistency**: ì¼ê´€ì„±ì´ ìˆëŠ”ì§€. some modified but some not, dangling, ...
> - **Timeliness**: ì–¼ë§ˆë‚˜ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ”ì§€. timely update?
> - **Believability**: how trustable the data are correct?
> - **Interpretability**: how easily the data can be understood?

<br/>

**í˜„ì‹¤ì˜ ë°ì´í„°ë“¤ì€ ë§¤ìš° ë‹¤ì–‘í•˜ê³  ë”ëŸ½ë‹¤. "Data in the Real World Is Dirty."**


: Lots of potentially incorrect data, 
e.g. instrument faulty(ê¸°ë¡í•˜ëŠ” ì¥ì¹˜ê°€ ì˜ëª»ëœ ê²½ìš°), human or computer error(ì¸ê°„ ë˜ëŠ” ì»´í“¨í„° ì˜¤ë¥˜), transmission err(ì „ì†¡ ì˜¤ë¥˜)


- **Incomplete**: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data.  
(ë°ì´í„°ë“¤ ì‚¬ì´ì— missing valueê°€ ì¡´ì¬)
	- e.g. Occupation=â€œ â€ (missing data)

- **Noisy**: containing noise, errors, or outliers  
(ë°ì´í„° ì¤‘ê°„ì— ì„ì—¬ìˆëŠ” ë…¸ì´ì¦ˆ, ì—ëŸ¬ ë˜ëŠ” ì´ìƒì¹˜.)
	- e.g. Salary=â€œâˆ’10â€ (an error)  

- **Inconsistent**: containing discrepancies in codes or names  
(ë‘ feature ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜.)
	- e.g. Age=â€œ42â€, Birthday=â€œ03/07/2010â€
	- Was rating â€œ1, 2, 3â€, now rating â€œA, B, Câ€
	- discrepancy between duplicate records  

- **Intentional**: ì˜ëª»ëœ ì¼ë°˜í™”.
	- e.g. 7th of July is everyone's birthday.  

**=> ì´ëŸ° ê²ƒë“¤ì„ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•´ì„œ ê¹”ë”í•œ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤.**

### Incomplete (Missing) Data

- Data is not always available
	- e.g. many tuples have no recorded value for several attributes, such as customer income in sales data
- Missing data may be due to
	- equipment malfunction
	- inconsistent with other recorded data and thus deleted   
	- data not entered due to misunderstanding
	- certain data may not be considered important at the time of entry
	- not register history or changes of the data
- Missing data may need to be inferred

#### Assumption for Missing Data

> **ğŸ“™ NOTE: Notation**
> - Missing or Not for variable Y: M = 0 (observed), M=1 (missing)
> - Other observed variable X

<br/>

- **Missing Completely at Random (MCAR)**

	- Missingness is independent of attributes and occurs entirely at random: $$P(Y\vert M) = P(Y)$$
	- Unrealistically strong assumption in practice

<br/>

- **Missing at Random (MAR)**

	- Missingness is related to other attributes (missingì´ ë ì§€ ë§ì§€ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤) : $$P(Y\vert X, M) = P(Y\vert X)$$
	- e.g. ì„¤ë¬¸ì¡°ì‚¬ì—ì„œ 25ì„¸ ë¯¸ë§Œì¸ ì‚¬ëŒë“¤ì˜ ì—°ë´‰ ì¹¸ì´ ëŒ€ì²´ë¡œ ë¹„ì–´ìˆë‹¤. -> 25ì„¸ ë¯¸ë§Œì€ í•™ìƒì´ê¸° ë•Œë¬¸ì— ì§ì—…ì´ ì—†ì–´ ì—°ë´‰ì´ ì—†ì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. (ìƒê´€ê´€ê³„)
	- The missingness can be explained by the variables that are actually observed
	- e.g. Missing data on hobbies tend to be more common among individuals with higher incomes

<br/>

- **Missing Not at Random (MNAR)**

	- Missingness is related to unobserved measurements (related to the reason it's missing) : $$P(Y\vert X, M) =Ì¸ P(Y\vert X)$$
	- Informative or non-ignorable missingness
	- e.g. Missing values in education level can imply lower education levels
	- missingëœ ê²ƒì´ ì–´ë– í•œ ë³€ìˆ˜ì— ì˜í•œ ì¦‰, ì›ì¸ì´ ìˆëŠ”ë° ê·¸ ì›ì¸ì´ ë˜ëŠ” ë³€ìˆ˜ê°€ ë°ì´í„°ì— ê¸°ë¡ì´ ë˜ì§€ ì•Šì•„ íŒŒì•…ì´ ì–´ë ¤ìš´ ê²½ìš°.

<br/>

![](https://i.imgur.com/Co8wqHO.png)

- y: Variable with missing values
- Blue: observed data
- Red: missing data

<br/>

#### How to Handle Missing Data?

- Eliminate data tuples or attributes (ë°ì´í„° ì‚­ì œ)
	- Usually done when class label is missing (when doing classification)â€”not effective when the % of missing values per attribute varies considerably
	- ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°ì— ê°€ì¥ í¸í•˜ë‹¤.
- Fill in the missing value manually: tedious + infeasible?
- Fill in it automatically with (ë°ì´í„° ì±„ìš°ê¸°)
	- a global constant : e.g., â€œunknownâ€, a new class?!
	- the attribute mean
	- the attribute mean for all samples belonging to the same class: smarter
	- **the most probable value: inference-based such as Bayesian formula or decision tree**

- Imputation Using **(Mean/Median) Values í‰ê· **
	- ğŸ‘ Pros :
		- Easy & Fast
		- Works well with small numerical datasets
	- ğŸ‘ Cons :
		- ë‹¨ì ì€ ë”±íˆ ì—†ì§€ë§Œ ì„±ëŠ¥ì´ ê·¸ë ‡ê²Œ ì¢‹ì§„ ì•Šë‹¤.

![](https://i.imgur.com/dIyna7L.png)

- Imputation Using **(Most Frequent) or (Zero/Constant) Values ê°€ì¥ ë¹ˆë„ê°€ ë§ì€ ê°’ ë˜ëŠ” 0**
	- ğŸ‘ Pros :
		- Works well with categorical features
	- ğŸ‘ Cons :
		- í° ë‹¨ì ì€ ë”±íˆ ì—†ë‹¤.

![](https://i.imgur.com/dZ4ezIT.png)

- Imputation Using **Inference-based Method (i.e., k-nn, multi-variate imputations, deep learning)** ë°ì´í„°ì˜ íŠ¹ì§•ë“¤ì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡
	- ğŸ‘ Pros :
		- Can be **much more accurate ê°€ì¥ ì •í™•í•˜ë‹¤** than the mean, median or most frequent imputation methods. (It depends on the dataset)
	- ğŸ‘ Cons :
		- **High computational costs with large datasets ëª¨ë¸ì„ ë˜ í•˜ë‚˜ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ìˆ˜ê³ ê°€ ìš”êµ¬ë¨**
		- You have to specify the columns that contain information about the target column that will be imputed

![](https://i.imgur.com/RSbV6Y0.png)

### Noisy Data

- **Noise** : random error or variance in a measured variable
- **Incorrect attribute values** may be due to
	- faulty data collection instruments
	- data entry problems
	- data transmission problems
	- technology limitation
	- inconsistency in naming convention
- **Other data problems** which require data cleaning
	- duplicate records
	- incomplete data
	- inconsistent data
=> **Data Quality**ì— ê´€í•œ ë¬¸ì œ!!!


#### How to Handle Noisy Data?

- **Binning êµ¬ê°„í™”**
	- first sort data and partition into (equal-frequency) bins
	- then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc. 
- **Regression íšŒê·€**
	- smooth by fitting the data into regression functions 
- **Clustering êµ°ì§‘í™”**
	- detect and remove outliers  
- **Combined computer and human inspection**
	- detect suspicious values and check by human (e.g., deal with possible outliers)

## Data Integration

- **Data integration ë°ì´í„° í†µí•©**:  
	- Combines data from multiple sources into a coherent store (ì—¬ëŸ¬ê°€ì§€ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ í•©ì¹˜ëŠ” ê²ƒ)
- Schema integration ìŠ¤í‚¤ë§ˆ í†µí•©:  
	- Integrate metadata from different sources
- **Entity identification problem**:  
	- Identify real world entities from multiple data sources (ê°ê°ì˜ sampleë“¤ì´ ì—¬ëŸ¬ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì˜¤ê¸° ë•Œë¬¸ì— ì´ë¦„ ê°™ì€ ê²ƒë“¤ì´ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.)
		- e.g. Bill Clinton = William Clinton
- **Detecting and resolving data value conflicts**
	- For the same real world entity, attribute values from different sources are different
	- Possible reasons: different representations, different scales
		- e.g. metric vs. British units

### Handling Redundancy in Data Integration (ë°ì´í„° í†µí•©ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ëŠ” ë°©ë²•)

- Redundant data occur often when integration of multiple databases
	- Object identification: The same attribute or object may have different names in different databases
	- Derivable data: One attribute may be a â€œderivedâ€ attribute in another table, e.g., annual revenue
- Redundant attributes may be able to be detected by correlation analysis and covariance analysis
- Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality

#### Correlation Analysis (Nominal Data)

- **$$x^2$$ (chi-square) test**
	- **ë‘ ê°œì˜ ë³€ìˆ˜ê°€ ìˆì„ ë•Œ, ê·¸ ë³€ìˆ˜ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì¸¡ê°’ë“¤ì˜ ë¶„í¬ë¥¼ ë¹„êµí•´ì„œ ì–¼ë§ˆë‚˜ ë‘ ê°œì˜ ë‹¤ë¥¸ categorical variableì´ ì„œë¡œ ê´€ë ¨ì´ ìˆëŠ”ì§€ë¥¼ ë³¸ë‹¤.**
	- The larger the $$x^2$$ value, the more likely the variables are related
	- The cells that contribute the most to the $$x^2$$ value are those whose actual count is very different from the expected count
$$x^2 = \sum{(observed - Expected)^2\over Expected}$$

- [**Correlation does not imply causality. ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ í•¨ì¶•í•˜ì§€ ì•ŠëŠ”ë‹¤.**](https://ko.wikipedia.org/wiki/%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84%EC%99%80_%EC%9D%B8%EA%B3%BC%EA%B4%80%EA%B3%84)
	- number of hospitals and number of car-theft in a city are correlated
	- Both are causally linked to the third variable: population

#### Chi-Square Calculation: An Example

|                          | Play chess | Not play chess | Sum (row) |
|:-------------------------|:----------:|:--------------:|:---------:|
| Like science fiction     |    250(90) |       200(360) |       450 |
| Not like science fiction |    50(210) |      1000(840) |      1050 |
| Sum(col.)                |        300 |           1200 |      1500 |  

$$
x^2 = {(250-90)^2 \over 90} + {(50-210)^2\over 210} + {(200-360)^2 \over 360} + {(1000-840)^2 \over 840} = 507.93
$$
- ì²« ë²ˆì§¸ ë³€ìˆ˜(: ì²´ìŠ¤ë¥¼ í•  ìˆ˜ ìˆëŠ”ì§€ ì—†ëŠ”ì§€)ì™€ ë‘ ë²ˆì§¸ ë³€ìˆ˜(: SFë¥¼ ì¢‹ì•„í•˜ëŠ”ì§€ ì¢‹ì•„í•˜ì§€ ì•ŠëŠ”ì§€)ì˜ ìƒê´€ê´€ê³„
- $x^2$ (chi-squared) calculation (numbers in parenthesis are expected counts calculated based on the data distribution in the two categories)

![](https://i.imgur.com/Dk0ybzB.png)

- Chi-squareëŠ” ììœ ë„ k(Degree of freedom)ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.
- It shows that like_science_fiction and play_chess are correlated in the group
$$
p(x^2>507.93) \approx 0
$$

#### Correlation Analysis (Numeric Data)

- Correlation coefficient (also called **Pearsonâ€™s product moment coefficient**)
	- $n$ = the number of tuples (ë°ì´í„°ì˜ ê°œìˆ˜), 
	- $\bar A$ and $\bar ğµ$  = the respective means of $A$ and $B$ ($A$ì™€ $B$ì˜ í‰ê· )
	- $\sigma_A$ and $\sigma_B$ = the respective standard deviation of $A$ and $B$ ($A$ì™€ $B$ì˜ í‘œì¤€í¸ì°¨)
	- $\sum(a_ib_i)$ = the sum of the $AB$ cross-product.
$$
r_{A,B} = \frac {\sum_{i=1}^{n}(a_i - \bar A)(b_i - \bar B)}{(n-1)\sigma_A\sigma_B} = \frac {\sum_{i=1}^{n}(a_ib_i) - n\bar A\bar B}{(n-1)\sigma_A\sigma_B}
$$
- If $r_{A,B} > 0$, $A$ and $B$ are positively correlated ($A$â€™s values increase as $B$â€™s). The higher, the stronger correlation.
- $r_{A,B} = 0$: independent (ë…ë¦½)
- $r_{A,B} < 0$: negatively correlated

#### Visually Evaluating Correlation

![](https://i.imgur.com/1FeVcAm.png)

#### Covariance (Numeric Data)

- Covariance is similar to correlation
$$
Cov(A,B) = E((A - \bar A)(B - \bar B)) = \frac {\sum_{i=1}^{n}(a_i-\bar A)(b_i - \bar B)}{n}
$$
$$
Cov(A,B) = E(A \cdot B) - \bar A \bar B
\qquad r_{A,B} = \frac{Cov(A, B)}{\sigma_A \sigma_B}
$$
- **Positive covariance**: If $Cov(A,B) > 0$, then $A$ and $B$ both tend to be larger than their expected values.
- **Negative covariance**: If $Cov(A,B) < 0$ then if $A$ is larger than its expected value, $B$ is likely to be smaller than its expected value.
- **Independence**: $Cov(A,B = 0)$ but the converse is not true

#### Covariance: An Example

- Suppose two stocks $A$ and $B$ have the following values in one week: (2, 5), (3, 8), (5, 10), (4, 11), (6, 14).
- Question: If the stocks are affected by the same industry trends, will their prices rise or fall together?
	- $E(A)=(2+3+5+4+6)/5=20/5=4$
	- $E(B)=(5+8+10+11+14)/5=48/5=9.6$
	- $Cov(A,B) = (2Ã—5+3Ã—8+5Ã—10+4Ã—11+6Ã—14)/5 âˆ’ 4 Ã— 9.6 = 4$
- Thus, $A$ and $B$ rise together since $Cov(A, B) > 0$.

## Data Reduction

### Data Reduction 1: Dimensionality Reduction

- **Curse of dimensionality ì°¨ì›ì˜ ì €ì£¼** 
	- **When dimensionality increases**, data becomes increasingly **sparse (í¬ì†Œí•´ì§„ë‹¤)**
	- Density and distance between points, which is critical to clustering, outlier analysis, becomes less meaningful
- **Dimensionality reduction ì°¨ì› ì¶•ì†Œ**
	- Avoid the curse of dimensionality
	- Help eliminate irrelevant features and reduce noise
	- Reduce time and space required in data mining
	- Allow easier visualization
- **Dimensionality reduction techniques ì°¨ì› ì¶•ì†Œ ê¸°ìˆ **
	- Wavelet transforms
	- Principal Component Analysis

#### Attribute Subset Selection

- One way to reduce dimensionality of data 
- **Redundant attributes ì¤‘ë³µëœ ë³€ìˆ˜ ì œê±°**
	- Duplicate much or all of the information contained in one or more other attributes
	- e.g., purchase price of a product and the amount of sales tax paid
- **Irrelevant attributes ìƒê´€ì—†ëŠ” ë³€ìˆ˜ ì œê±°**
	- Contain no information that is useful for the **data mining task** at hand
	- e.g., students' ID is often irrelevant to the task of predicting students' GPA

### Data Reduction 2: Numerosity Reduction

- **Clustering**
	- Partition data set into clusters based on similarity, and store cluster representation (e.g., centroid and diameter) only
	- Can be very effective if data is clustered but not if data is â€œsmearedâ€
	- Can have hierarchical clustering and be stored in multi-dimensional index tree structures
- **Sampling**
	- Obtaining a small sample s to represent the whole data set $N$
	- Sampling is typically used in data mining when processing the entire set of data of interest is too expensive or time consuming.
	- Key principles for effective sampling are:
		- A sample is representative if it has approximately the same properties (of interest) as the original data set.
		- If a sample is representative, using the sample will work almost as well as using the entire data set.

#### Types of Sampling

- **Simple Random Sampling**
	- There is an equal probability of selecting any particular object
	- **Sampling without replacement ë¹„ë³µì› ì¶”ì¶œ**
		- As each object is selected, it is removed from the population
	- **Sampling with replacement ë³µì› ì¶”ì¶œ**
		- Objects are not removed from the population as they are selected for the sample
		- The same object can be picked up more than once
	- Simple random sampling may have very poor performance in the presence of skew
- **Stratified Sampling**
	- Split the data into several partitions; then draw a random sample from each partition (ë°ì´í„°ë¥¼ ë½‘ì„ ë•Œ, íŒŒí‹°ì…˜ì„ ë‚˜ëˆ ì„œ ë½‘ëŠ” ìƒ˜í”Œë§)

#### Sampling: With or without Replacement

![](https://i.imgur.com/sH6jXbV.png)

#### Sampling: Stratified Sampling

![](https://i.imgur.com/ypVPJli.png)

## Data Transformation

- A function that maps the entire set of values of **a given attribute** to a new set of replacement values s.t. each old value can be identified with one of the new values
- Methods
	- **Smoothing**: Remove noise from data (í‰ê· ì„ ì ìš©í•´ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ê²ƒ - ex.ì´ë™í‰ê· )
	- Attribute/feature construction
		- New attributes constructed from the given ones
	- Simple function: $xk$, $\log(1+x)$, $e^x$, $\left \vert x \right \vert$, $\dots$ 
		- ê¸ˆìœµ ë°ì´í„°ì˜ ê²½ìš°, ìˆ˜ì¹˜ ê·¸ ìì²´ë³´ë‹¤ ë¹„ìœ¨ì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ëŒ€ë¶€ë¶„ $\log$ ë¥¼ ì·¨í•œë‹¤.
	- **Normalization ì •ê·œí™”**: Scaled to fall within a smaller, specified range (ê°’ì„ ì–´ë–¤ ë²”ìœ„ ì•ˆì— ë„£ì–´ ì£¼ëŠ” ê²ƒ - ëª¨ë“  ë°ì´í„°ë¥¼ ì¼ê´€ì ì¸ í˜•íƒœë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•¨.)
		- min-max normalization
		- z-score normalization
		- normalization by decimal scaling
	- **Discretization**: Concept hierarchy climbing (ì—°ì†ì ì¸ ë³€ìˆ˜ë¥¼ ì˜ë¼ì„œ ë‚˜ëˆ„ëŠ” ê²ƒ)

### Normalization

- **Min-max normalization**: to $newmin_A$, $newmax_A$
	- Ex. Let income range 12,000 to 98,000 normalized to $[0.0, 1.0]$
	- The 73,000 is mapped to $\frac{73600âˆ’12000}{98000-12000} (1 âˆ’ 0) + 0 = 0.716$
$$
v\prime = \frac {v-min_A}{max_A - min_A} (newmax_A - newmin_A)+newmin_A
$$
- $v$ ë¼ëŠ” ê°’ì— ë‚´ê°€ ê°€ì§„ attributeì˜ ê°€ì¥ ì‘ì€ $min$ ê°’ì„ ë¹¼ì£¼ê³ , $max - min$ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì£¼ë©´ ëª¨ë“  ê°’ì´ 0~1 ì‚¬ì´ë¡œ ê°€ê²Œ ëœë‹¤. ëª¨ë“  ê°’ë“¤ì„ ê°€ì¥ í° ê°’ì„ 1ë¡œ ë‘ê³ , ê°€ì¥ ì‘ì€ ê°’ì„ 0ìœ¼ë¡œ ë‘ê³ , ê·¸ê²ƒì„ linearí•˜ê²Œ ê¾¸ë¯¸ëŠ” í˜•íƒœê°€ normalization ì •ê·œí™”ì´ë‹¤.
- **Z-score normalization** ($\mu$: mean, $\sigma$: standard deviation):
	- Ex. Let $\mu=54,000$ , $\sigma=16,000$ . Then, $\frac{73600-54000}{16000}=1.225$
$$
v\prime = \frac{v - \mu_A}{\sigma_A}
$$
=> ex. í‘œì¤€ì ìˆ˜ (í‘œì¤€í¸ì°¨ë¡œ ë³€í™˜í•´ì„œ ë‚´ê°€ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë§ì´ ì•ì„œìˆëŠ”ì§€($\sigma$) ì•Œì•„ë³¸ë‹¤)

### Discretization

- **Three types of attributes**
	- Nominal â€” values from an unordered set, e.g., color, profession
	- Ordinal â€” values from an ordered set, e.g., military or academic rank
	- Numeric â€” real numbers, e.g., integer or real numbers
- **Discretization: Divide the range of a continuous attribute into intervals**  
  (ì–´ë– í•œ ê¸°ì¤€ì„ ê°€ì§€ê³  ë‚˜ëˆ„ëŠ”ì§€ê°€ ì¤‘ìš”í•œ ê´€ê±´ì´ ëœë‹¤)
	- Interval labels can then be used to replace actual data values
	- Reduce data size by discretization
	- **Supervised vs. unsupervised**
	- Split (top-down) vs. merge (bottom-up)
	- Discretization can be performed recursively on an attribute
	- Prepare for further analysis, e.g., classification  
	
	**=> ë°ì´í„°ë¥¼ ë‹¨ìˆœí™”ì‹œí‚¤ê³  ì‹¶ì„ ë•Œ ë§ì´ ì“°ëŠ” ë°©ë²•**
	
#### Simple Discretization: Binning

- **Equal-width (distance)** partitioning
	- Divides the range into N intervals of equal size: uniform grid
	- if $A$ and $B$ are the lowest and highest values of the attribute, the width of intervals will be: $W = (B â€“A)/N$
	- The most straightforward, but outliers may dominate presentation
	- Skewed data is not handled well
- **Equal-depth (frequency)** partitioning
	- Divides the range into N intervals, each containing approximately same number of samples
	- Good data scaling
	- Managing categorical attributes can be tricky

#### Binning Methods for Data Smoothing

- Sorted data for price (in dollars): 4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34 
* Partition into **equal-frequency (equi-depth)** bins:
	* Bin 1: 4, 8, 9, 15  
	- Bin 2: 21, 21, 24, 25
	- Bin 3: 26, 28, 29, 34
* Smoothing by **bin means**:
	- Bin 1: 9, 9, 9, 9  
	- Bin 2: 23, 23, 23, 23  
	- Bin 3: 29, 29, 29, 29
* Smoothing by **bin boundaries**:
	- Bin 1: 4, 4, 4, 15  
	- Bin 2: 21, 21, 25, 25  
	- Bin 3: 26, 26, 26, 34

![](https://i.imgur.com/5IIrmW5.png)

### Binarization (One-hot-encoding)

- Binarization **maps a continuous or categorical attribute into one or more binary attributes**
	- Often convert a continuous attribute to a categorical attribute and then convert the categorical attribute to a set of binary attributes
	- Typically used for association analysis

![](https://i.imgur.com/LQBHFEg.png)

## Data Exploration

### Summary Statistics

- Summary statistics are numbers that summarize properties of the data
	- For **categorical attributes**
		- Frequency, Mode
	- For **continuous attributes**
		- Percentiles
		- Measures of Location: Mean and Median
		- Measures of Spread: Range and Variance

### Histograms

- Used to plot the distribution of the values of **a single attribute** (**í•œ ê°€ì§€ ë³€ìˆ˜**ê°€ ì–´ë–»ê²Œ ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€)
	- It divide the values into bins and shows a bar plot of the number of objects in each bin
	- The height of each bar indicates the number of objects
	- Shape of histogram depends on **the number of bins** (**$x$ì¶•ì„ ì–¼ë§ˆë‚˜ ì˜ê²Œ ë‚˜ëˆŒì§€ ê²°ì •í•˜ëŠ” ìš”ì†Œ**ì¸ bin ê°œìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.)

![](https://i.imgur.com/ScSloAn.png)

- Two-Dimensional Histograms
	- Used to plot the joint distribution of the values of two attributes

![](https://i.imgur.com/N9aHC77.png)

### Box Plots

- Another way of displaying the **distribution ë¶„í¬ of data**
- Box plots can be used to **compare ë¹„êµ attributes**

![](https://i.imgur.com/oN3sEB7.png)

![](https://i.imgur.com/HOtRLKc.png)

- 50th percentile : 100ë¶„ìœ„ìˆ˜ê°€ 50 (=median ì¤‘ì•™ê°’)
- 25th, 75th percentile : 4ë¶„ìœ„ìˆ˜

### Scatter Plots

- Used to plot the **pairwise correlations** between attributes (**ë‘ ê°€ì§€ ë³€ìˆ˜**ê°€ ì–´ë– í•œ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€)
	- Each data object is depicted as a marker.
	- The position of a marker is determined by the values of its attributes.
	- While two-dimensional scatter plots are most common, three-dimensional scatter plots can also be utilized.
	- Often, additional attributes can be displayed by using the size, shape, and color of the markers that represent the objects
- Additional attributes can be incorporated into scatter plots through:
	- **Marker Size**: Representing an attribute using the size of markers.
	- **Marker Shape**: Differentiating markers based on an additional attribute.
	- **Marker Color**: Using color variation to convey information about another attribute.

![CvkfoFP.png](https://i.imgur.com/CvkfoFP.png)

- petal_lengthì™€ petal_widthëŠ” ì„ í˜•ì  ê´€ê³„ê°€ ìˆìŒ. (ë¹„ë¡€í•˜ëŠ” í˜•íƒœì˜ ë°ì´í„° êµ¬ì„±)
- sepal_widthì™€ sepal_lengthëŠ” ê´€ë ¨ì´ ì—†ë‹¤.

